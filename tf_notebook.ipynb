{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils import play_game2\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "from agents.agent import Agent\n",
    "from agents.agent import mean_huber_loss\n",
    "from game_environment import SnakeNumpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "version = 'v17.1'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class DeepQLearningAgent(Agent):\n",
    "    def __init__(self, board_size, frames, buffer_size, n_actions, version, use_target_net=True, gamma=0.99):\n",
    "        super().__init__(board_size, frames, buffer_size, gamma, n_actions, use_target_net, version)\n",
    "        self._model = self.model()\n",
    "        self._target_net = self._model\n",
    "        self.update_target()\n",
    "\n",
    "\n",
    "    def model(self):\n",
    "        with open('model_config/v17.1.json', 'r') as f:\n",
    "            m = json.loads(f.read())\n",
    "\n",
    "        input_board = Input((self._board_size, self._board_size, self._n_frames,), name='input')\n",
    "        x = input_board\n",
    "        for layer in m['model']:\n",
    "            l = m['model'][layer]\n",
    "            if 'Conv2D' in layer:\n",
    "                # add convolutional layer\n",
    "                x = Conv2D(**l)(x)\n",
    "            if 'Flatten' in layer:\n",
    "                x = Flatten()(x)\n",
    "            if 'Dense' in layer:\n",
    "                x = Dense(**l)(x)\n",
    "        out = Dense(self._n_actions, activation='linear', name='action_values')(x)\n",
    "        model = Model(inputs=input_board, outputs=out)  # Keras model\n",
    "        model.compile(optimizer=RMSprop(0.0005), loss=mean_huber_loss)\n",
    "        return model\n",
    "\n",
    "    def update_target(self):\n",
    "        self._target_net.set_weights(self._model.get_weights())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "board_size = 10\n",
    "frames = 2\n",
    "max_time_limit = 998\n",
    "supervised = False\n",
    "n_actions = 4\n",
    "obstacles = False\n",
    "buffer_size = 80000\n",
    "\n",
    "frame_mode = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "agent = DeepQLearningAgent(board_size=board_size, frames=frames, buffer_size=buffer_size, n_actions=n_actions, version=version)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# some more funny parameters\n",
    "epsilon, epsilon_end = 1, 0.01\n",
    "reward_type = 'current'\n",
    "sample_actions = False\n",
    "n_games_training = 8 * 16\n",
    "games_eval = 8\n",
    "decay = 0.97\n",
    "episodes = 2 * (10 ** 5)\n",
    "log_frequency = 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing 32768 frames took 0.31s\n"
     ]
    }
   ],
   "source": [
    "# play some games to fill buffer\n",
    "games = 512\n",
    "env = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                 max_time_limit=max_time_limit, games=games,\n",
    "                 frame_mode=True, obstacles=obstacles, version=version)\n",
    "ct = time.time()\n",
    "_ = play_game2(env, agent, n_actions, n_games=games, record=True,\n",
    "                   epsilon=epsilon, verbose=True, reset_seed=False,\n",
    "                   frame_mode=True, total_frames=games * 64)\n",
    "print('Playing {:d} frames took {:.2f}s'.format(games * 64, time.time() - ct))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "env = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                 max_time_limit=max_time_limit, games=n_games_training,\n",
    "                 frame_mode=True, obstacles=obstacles, version=version)\n",
    "env2 = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                  max_time_limit=max_time_limit, games=games_eval,\n",
    "                  frame_mode=True, obstacles=obstacles, version=version)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training loop\n",
    "model_logs = {'iteration': [], 'reward_mean': [],\n",
    "              'length_mean': [], 'games': [], 'loss': []}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index in tqdm(range(episodes)):\n",
    "    # make small changes to the buffer and slowly train\n",
    "        _, _, _ = play_game2(env, agent, n_actions, epsilon=epsilon,\n",
    "                             n_games=n_games_training, record=True,\n",
    "                             sample_actions=sample_actions, reward_type=reward_type,\n",
    "                             frame_mode=True, total_frames=n_games_training,\n",
    "                             stateful=True)\n",
    "        loss = agent.train_agent(batch_size=64,\n",
    "                                 num_games=n_games_training, reward_clip=True)\n",
    "\n",
    "     # check performance every once in a while\n",
    "        if (index + 1) % log_frequency == 0:\n",
    "            current_rewards, current_lengths, current_games = \\\n",
    "                play_game2(env2, agent, n_actions, n_games=games_eval, epsilon=-1,\n",
    "                           record=False, sample_actions=False, frame_mode=True,\n",
    "                           total_frames=-1, total_games=games_eval)\n",
    "\n",
    "            model_logs['iteration'].append(index + 1)\n",
    "            model_logs['reward_mean'].append(round(int(current_rewards) / current_games, 2))\n",
    "            model_logs['length_mean'].append(round(int(current_lengths) / current_games, 2))\n",
    "            model_logs['games'].append(current_games)\n",
    "            model_logs['loss'].append(loss)\n",
    "            pd.DataFrame(model_logs)[['iteration', 'reward_mean', 'length_mean', 'games', 'loss']] \\\n",
    "                .to_csv('model_logs/{:s}.csv'.format(version), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}